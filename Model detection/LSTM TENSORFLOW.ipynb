{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "285/285 [==============================] - 5s 7ms/step - loss: 1.1380 - accuracy: 0.5390 - val_loss: 0.9094 - val_accuracy: 0.6357\n",
      "Epoch 2/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.7971 - accuracy: 0.6813 - val_loss: 0.7863 - val_accuracy: 0.6735\n",
      "Epoch 3/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.7152 - accuracy: 0.7039 - val_loss: 0.7419 - val_accuracy: 0.6840\n",
      "Epoch 4/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6865 - accuracy: 0.7113 - val_loss: 0.7266 - val_accuracy: 0.6878\n",
      "Epoch 5/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6682 - accuracy: 0.7164 - val_loss: 0.7101 - val_accuracy: 0.6961\n",
      "Epoch 6/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.7231 - val_loss: 0.6963 - val_accuracy: 0.6983\n",
      "Epoch 7/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.7290 - val_loss: 0.6888 - val_accuracy: 0.7058\n",
      "Epoch 8/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6313 - accuracy: 0.7313 - val_loss: 0.6783 - val_accuracy: 0.7073\n",
      "Epoch 9/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.7374 - val_loss: 0.6715 - val_accuracy: 0.7170\n",
      "Epoch 10/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6137 - accuracy: 0.7420 - val_loss: 0.6670 - val_accuracy: 0.7205\n",
      "Epoch 11/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6063 - accuracy: 0.7452 - val_loss: 0.6555 - val_accuracy: 0.7183\n",
      "Epoch 12/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.6011 - accuracy: 0.7490 - val_loss: 0.6518 - val_accuracy: 0.7256\n",
      "Epoch 13/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5952 - accuracy: 0.7499 - val_loss: 0.6516 - val_accuracy: 0.7249\n",
      "Epoch 14/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5895 - accuracy: 0.7531 - val_loss: 0.6461 - val_accuracy: 0.7293\n",
      "Epoch 15/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5859 - accuracy: 0.7550 - val_loss: 0.6387 - val_accuracy: 0.7324\n",
      "Epoch 16/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5801 - accuracy: 0.7574 - val_loss: 0.6352 - val_accuracy: 0.7394\n",
      "Epoch 17/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.7569 - val_loss: 0.6384 - val_accuracy: 0.7343\n",
      "Epoch 18/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5710 - accuracy: 0.7618 - val_loss: 0.6365 - val_accuracy: 0.7357\n",
      "Epoch 19/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5677 - accuracy: 0.7632 - val_loss: 0.6302 - val_accuracy: 0.7392\n",
      "Epoch 20/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5650 - accuracy: 0.7643 - val_loss: 0.6251 - val_accuracy: 0.7363\n",
      "Epoch 21/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5618 - accuracy: 0.7674 - val_loss: 0.6250 - val_accuracy: 0.7431\n",
      "Epoch 22/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5586 - accuracy: 0.7682 - val_loss: 0.6191 - val_accuracy: 0.7423\n",
      "Epoch 23/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5546 - accuracy: 0.7703 - val_loss: 0.6214 - val_accuracy: 0.7423\n",
      "Epoch 24/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5535 - accuracy: 0.7711 - val_loss: 0.6230 - val_accuracy: 0.7412\n",
      "Epoch 25/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5485 - accuracy: 0.7722 - val_loss: 0.6141 - val_accuracy: 0.7456\n",
      "Epoch 26/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5456 - accuracy: 0.7753 - val_loss: 0.6147 - val_accuracy: 0.7477\n",
      "Epoch 27/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5430 - accuracy: 0.7745 - val_loss: 0.6138 - val_accuracy: 0.7473\n",
      "Epoch 28/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5411 - accuracy: 0.7738 - val_loss: 0.6176 - val_accuracy: 0.7453\n",
      "Epoch 29/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5394 - accuracy: 0.7764 - val_loss: 0.6134 - val_accuracy: 0.7453\n",
      "Epoch 30/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5371 - accuracy: 0.7776 - val_loss: 0.6108 - val_accuracy: 0.7519\n",
      "Epoch 31/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5337 - accuracy: 0.7789 - val_loss: 0.6115 - val_accuracy: 0.7550\n",
      "Epoch 32/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5328 - accuracy: 0.7820 - val_loss: 0.6108 - val_accuracy: 0.7493\n",
      "Epoch 33/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5299 - accuracy: 0.7829 - val_loss: 0.6136 - val_accuracy: 0.7488\n",
      "Epoch 34/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5283 - accuracy: 0.7825 - val_loss: 0.6065 - val_accuracy: 0.7524\n",
      "Epoch 35/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5264 - accuracy: 0.7819 - val_loss: 0.6066 - val_accuracy: 0.7488\n",
      "Epoch 36/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5240 - accuracy: 0.7845 - val_loss: 0.6092 - val_accuracy: 0.7535\n",
      "Epoch 37/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.7862 - val_loss: 0.6030 - val_accuracy: 0.7576\n",
      "Epoch 38/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5207 - accuracy: 0.7875 - val_loss: 0.6090 - val_accuracy: 0.7528\n",
      "Epoch 39/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5182 - accuracy: 0.7870 - val_loss: 0.6054 - val_accuracy: 0.7546\n",
      "Epoch 40/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7870 - val_loss: 0.6083 - val_accuracy: 0.7570\n",
      "Epoch 41/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5151 - accuracy: 0.7891 - val_loss: 0.6036 - val_accuracy: 0.7552\n",
      "Epoch 42/50\n",
      "285/285 [==============================] - 1s 4ms/step - loss: 0.5144 - accuracy: 0.7921 - val_loss: 0.6078 - val_accuracy: 0.7563\n",
      "Model and preprocessors saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the data\n",
    "file_paths = [\n",
    "    'bag_toss_labeled_1.csv',\n",
    "    'sitting_up_down_labeled_2.csv',\n",
    "    'stairs_labeled_4.csv',\n",
    "    'walking_labeled_3.csv'\n",
    "]\n",
    "data_frames = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "# Preprocess the data\n",
    "data_for_model = combined_data.drop(columns=['timestamp'])  # Assuming timestamp is not used\n",
    "X = data_for_model.drop(columns=['label'])\n",
    "y = data_for_model['label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_lstm, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping], batch_size=64)\n",
    "\n",
    "# Save the model and preprocessors\n",
    "model.save('lstm_model.h5')\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "print(\"Model and preprocessors saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
